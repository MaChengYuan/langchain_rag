{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "a39db5bf-8d35-43bb-b2b4-5dd928660c40",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import cuda, bfloat16\n",
    "import transformers\n",
    "from transformers import AutoTokenizer\n",
    "from time import time\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.vectorstores import Chroma\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "56d2b833-df2e-4ddf-b116-1a6b2de51bab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import TextLoader,DirectoryLoader # 文本加载器\n",
    "from langchain.text_splitter import CharacterTextSplitter # 文本分块器\n",
    "from langchain_community.embeddings import OllamaEmbeddings # Ollama向量嵌入器\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain.prompts import ChatPromptTemplate # 聊天提示模板\n",
    "from langchain_community.chat_models import ChatOllama # ChatOllma聊天模型\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "from langchain.schema.output_parser import StrOutputParser # 输出解析器\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "d0542086-9839-4bb6-9cb8-dea59d866379",
   "metadata": {},
   "outputs": [],
   "source": [
    "# change url 網址\n",
    "url = \"https://en.itmo.ru/en/viewjep/2/5/Big_Data_and_Machine_Learning.htm\"\n",
    "res = requests.get(url)\n",
    "with open(\"state_of_the_union.txt\", \"w\") as f:\n",
    "    f.write(res.text)\n",
    "# 加载文件\n",
    "loader = TextLoader('./state_of_the_union.txt')\n",
    "documents = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "e0915c3e-60c6-4000-aadc-7d3269afd32c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#webpage loader\n",
    "from langchain.document_loaders import WebBaseLoader\n",
    "\n",
    "loader = WebBaseLoader(url)\n",
    "documents = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "0e619ce1-2aaf-48eb-b73a-4fe0661c4b15",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = CharacterTextSplitter(chunk_size=3000, chunk_overlap=50)\n",
    "new_docs = text_splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "3d3c8555-afb5-46a6-94e0-c4578af78275",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # in persistent db\n",
    "# import chromadb\n",
    "# from chromadb.config import Settings\n",
    "\n",
    "\n",
    "# client = chromadb.Client(Settings(chroma_db_impl=\"duckdb+parquet\",\n",
    "#                                     persist_directory=\"db/\"\n",
    "#                                 ))\n",
    "# collection = client.create_collection(name=\"Students\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "17880c07-053b-43b9-84ac-5729761fcdcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# embeddings = HuggingFaceEmbeddings(\n",
    "#     model_name='sentence-transformers/all-mpnet-base-v2'\n",
    "#     #model_name='roberta-large'\n",
    "# )\n",
    "# db = Chroma.from_documents(new_docs, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "13a8c6bb-b3a8-41fb-bec8-0386bb83bf55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show embeddings vector instances\n",
    "#embeddings.embed_query('this is an embedding')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "804d8110-0a62-4532-91ed-6acc4b994792",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "embeddings = HuggingFaceEmbeddings()\n",
    "db = Chroma.from_documents(new_docs, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "78b5f952-09a4-408b-8343-15674f80b3e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain_community.embeddings import OllamaEmbeddings\n",
    "# embeddings = OllamaEmbeddings(model=\"llama3\")\n",
    "# db = Chroma.from_documents(new_docs, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "3d17b3fd-90a6-4706-84b3-4f3e839031ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# query only from db\n",
    "\n",
    "query = \"what are classes in first semester\"\n",
    "retriever = db.as_retriever(k=5) # can add mmr fetch_k=20, search_type=\"mmr\"\n",
    "\n",
    "result = retriever.get_relevant_documents(query)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "e1e35b4a-a584-45bf-b720-5b5c042f5027",
   "metadata": {},
   "outputs": [],
   "source": [
    "# self-defined reranker\n",
    "def reranker(query, hits):\n",
    "    from sentence_transformers import CrossEncoder\n",
    "    \n",
    "    # To refine the results, we use a CrossEncoder. A CrossEncoder gets both inputs (input_question, retrieved_question)\n",
    "    # and outputs a score 0...1 indicating the similarity.\n",
    "    cross_encoder_model = CrossEncoder(\"cross-encoder/stsb-roberta-base\")\n",
    "\n",
    "    # Now, do the re-ranking with the cross-encoder\n",
    "    sentence_pairs = [[query, hit.page_content] for hit in hits]\n",
    "    similarity_scores = cross_encoder_model.predict(sentence_pairs)\n",
    "    ranking = np.argsort(similarity_scores)\n",
    "    \n",
    "    rerank_result = []\n",
    "    for i in ranking:\n",
    "        rerank_result.append(result[i])\n",
    "    return rerank_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "866abb20-038c-4db2-b977-6c94b8eb5ca4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Model type could not be auto-mapped with the defaults list. Defaulting to TransformerRanker.\n",
      "If your model is NOT intended to be ran as a one-label cross-encoder, please reload it and specify the model_type! Otherwise, you may ignore this warning. You may specify `model_type='cross-encoder'` to suppress this warning in the future.\n",
      "Loading TransformerRanker model mixedbread-ai/mxbai-rerank-base-v1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mac/Library/Python/3.9/lib/python/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# rerank\n",
    "\n",
    "from rerankers import Reranker\n",
    "from langchain.retrievers.contextual_compression import ContextualCompressionRetriever\n",
    "\n",
    "ranker = Reranker(\"mixedbread-ai/mxbai-rerank-base-v1\", verbose=0)\n",
    "compressor = ranker.as_langchain_compressor(k=3)\n",
    "\n",
    "compression_retriever = ContextualCompressionRetriever(\n",
    "    base_compressor=compressor, base_retriever=retriever\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "644a638f-b39c-4baf-b613-4c7888568064",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = db.as_retriever()\n",
    "# LLM提示模板\n",
    "template = \"\"\"You are an assistant for question-answering tasks. \n",
    "   Use the following pieces of retrieved context to answer the question. \n",
    "   If you don't know the answer, just say that you don't know. \n",
    "   Use three sentences maximum and keep the answer concise.\n",
    "   Question: {question} \n",
    "   Context: {context} \n",
    "   Answer:\n",
    "   \"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "5643b5f7-58c6-4211-b320-c28a4a5a7647",
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete \n",
    "# db.delete_collection()\n",
    "# get items in chromadb\n",
    "# db.get()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "f98a9414-49ad-4f8d-adfb-fcbfa31d6146",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In the second semester, classes include Academic English, Applied Artificial Intelligence Part 2, Soft Skills elective discipline, Methodology of Translational Research, Machine Learning, Architecture of Neural Networks for Deep Learning, Technologies and Infrastructure for Big Data, Discrete Modeling/Data Visualization, and Research Internship. These classes cover a range of topics related to data analysis, artificial intelligence, and research methodologies.\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    model=\"gpt-3.5-turbo-0125\",\n",
    "        temperature=0,\n",
    "        max_tokens=800,\n",
    "        model_kwargs={\"top_p\": 0, \"frequency_penalty\": 0, \"presence_penalty\": 0},\n",
    "    api_key=\"sk-yH3c6UN5bNPTEuF7haYlC3a8mpyRuPp28zp5poGyZnUfnZCp\",\n",
    "    base_url=\"https://api.chatanywhere.tech/v1\"\n",
    "    )\n",
    "rag_chain = (\n",
    "        {\"context\": compression_retriever, \"question\": RunnablePassthrough()} # 上下文信息\n",
    "        | prompt\n",
    "        | llm\n",
    "        | StrOutputParser()\n",
    ")\n",
    "# generation\n",
    "query = \"what are classes in second semester\"\n",
    "print(rag_chain.invoke(query))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "04104fd6-d022-4207-89c4-6217b358a73f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In the second semester, classes include Academic English, Applied Artificial Intelligence, Soft Skills elective discipline, Methodology of Translational Research, Machine Learning, Architecture of Neural Networks for Deep Learning, Technologies and Infrastructure for Big Data, and Research Internship.\n"
     ]
    }
   ],
   "source": [
    "rag_chain = (\n",
    "        {\"context\": retriever, \"question\": RunnablePassthrough()} # 上下文信息\n",
    "        | prompt\n",
    "        | llm\n",
    "        | StrOutputParser()\n",
    ")\n",
    "# 开始查询&生成\n",
    "query = \"what are classes in second semester\"\n",
    "print(rag_chain.invoke(query))\n",
    "\n",
    "\n",
    "# using llama as generative model\n",
    "# llm = ChatOllama(model=\"llama3\", temperature=0)\n",
    "# rag_chain = (\n",
    "#         {\"context\": retriever, \"question\": RunnablePassthrough()} # 上下文信息\n",
    "#         | prompt\n",
    "#         | llm\n",
    "#         | StrOutputParser()\n",
    "# )\n",
    " \n",
    "# query = \"what are classes in first semester\"\n",
    "# print(rag_chain.invoke(query))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57f31d72-b6f5-4260-bdf4-c0a777b26245",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "522205df-99ab-420a-8de8-8625bdf3d38a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15156fb2-634c-4eba-892d-aaaff1f0ef8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test for customized embedding model from hugging face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6cd75243-70f5-4983-a191-45b85b7c7b76",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mac/Library/Python/3.9/lib/python/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.llms.huggingface_pipeline import HuggingFacePipeline\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline, AutoModelForSeq2SeqLM\n",
    "\n",
    "model_id = \"gpt2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id)\n",
    "pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer, max_new_tokens=10)\n",
    "hf = HuggingFacePipeline(pipeline=pipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "27cd7faf-2d80-4fc4-a796-48b5530d902c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mac/Library/Python/3.9/lib/python/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# encoder to decoder\n",
    "# not working\n",
    "#model_id = 'google/flan-t5-large'# go for a smaller model if you dont have the VRAM\n",
    "model_id = 't5-base'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_id)\n",
    "\n",
    "pipe = pipeline(\n",
    "    \"text2text-generation\",\n",
    "    model=model, \n",
    "    tokenizer=tokenizer, \n",
    "    max_length=100\n",
    ")\n",
    "\n",
    "hf = HuggingFacePipeline(pipeline=pipe)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "064db99c-655b-4f5a-bc33-ab602e9593e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mac/Library/Python/3.9/lib/python/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# works\n",
    "\n",
    "from langchain_community.llms.huggingface_pipeline import HuggingFacePipeline\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "\n",
    "model_id = \"gpt2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id)\n",
    "pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer, max_new_tokens=10)\n",
    "hf = HuggingFacePipeline(pipeline=pipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "35781e8e-6ea8-4f26-a72a-6cbf1e5e7329",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "template = \"\"\"Question: {question}\n",
    "\n",
    "Answer: Let's think step by step.\"\"\"\n",
    "prompt = PromptTemplate.from_template(template)\n",
    "\n",
    "chain = prompt | hf\n",
    "\n",
    "question = \"What is electroencephalography?\"\n",
    "\n",
    "print(chain.invoke({\"question\": question}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27344f91-9f76-4c31-aa35-4a855821dc43",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
